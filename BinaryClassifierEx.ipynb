{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optional-coach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from random import randint\n",
    "\n",
    "class BinDataset(Dataset):\n",
    "     def __init__(self, arr=[0,1]):\n",
    "         super(Dataset, self).__init__()\n",
    "         self.arr = arr\n",
    "         \n",
    "        \n",
    "     def __len__(self): # ovo se mora definirati\n",
    "         return len(self.arr)\n",
    "        \n",
    "     def __getitem__(self, index): # i ovo za uzeti  data, truth\n",
    "        tensor = torch.tensor((self.arr[index]), dtype = torch.float, requires_grad = True)\n",
    "        tensor = torch.tensor((self.arr[index]), dtype = torch.float, requires_grad = False)\n",
    "        return tensor.unsqueeze(0), tensor.unsqueeze(0)\n",
    "#         return torch.FloatTensor(self.arr[index]), torch.FloatTensor(self.arr[index])\n",
    "\n",
    "arr = [randint(0, 1) for p in range(0, 20)]     \n",
    "\n",
    "# print(arr)\n",
    "\n",
    "data_train = BinDataset(arr=arr)\n",
    "data_train_loader = DataLoader(data_train, batch_size=5, shuffle=True)\n",
    "\n",
    "# print(len(data_train), data_train[0][0])\n",
    "# print(len(data_train_loader))\n",
    "\n",
    "print(len(data_train))\n",
    "# print(torch.tensor([1], dtype=torch.int).shape)\n",
    "# print(torch.tensor([1], dtype=torch.int))\n",
    "# print(torch.tensor(1, dtype=torch.int))\n",
    "# for (x,y) in data_train:\n",
    "#     print(x,y)\n",
    "#     print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-rings",
   "metadata": {},
   "source": [
    "Input size 1\n",
    "hidden 3\n",
    "Output size 1\n",
    "\n",
    "When you have more than two hidden layers, the model is also called the deep/multilayer feedforward model or multilayer perceptron model(MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "optimum-franchise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "tensor([0.4253]) tensor([0.], grad_fn=<AbsBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1,hidden_size=3):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc = nn.Linear(in_channels,in_channels) # fully connected linear layer\n",
    "        self.relu = torch.nn.ReLU()  # step function / activation function\n",
    "        # After every hidden layer, an activation function is applied to introduce non-linearity\n",
    "        self.fc1 = torch.nn.Linear(in_channels, hidden_size) # fully connected linear layer\n",
    "        self.relu = torch.nn.ReLU() # step function / activation function\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, out_channels) # fully connected linear layer\n",
    "        self.sigmoid = torch.nn.Sigmoid() # step function / activation function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "#         x = self.relu(x)         \n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         print(x)\n",
    "        return torch.abs(torch.round(x))\n",
    "\n",
    "\n",
    "x = torch.randn((1))\n",
    "model = Network(in_channels=1, out_channels=1,hidden_size=3)\n",
    "preds = model(x)\n",
    "print(preds.shape)\n",
    "print(x.shape)\n",
    "assert x.shape == preds.shape\n",
    "print(x,preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-headquarters",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "smart-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 1999.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.5411929488182068\n",
      "Epoch 1: train loss: 0.6171700358390808\n",
      "Epoch 2: train loss: 0.5411929488182068\n",
      "Epoch 3: train loss: 0.38923874497413635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = Network(1, 1, 3)\n",
    "criterion = torch.nn.SoftMarginLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "model.eval() # sets the PyTorch module to evaluation mode\n",
    "# We want to do this because we don’t want the model to learn new weights\n",
    "# when we just want to check the loss before training. \n",
    "# To train, we switch the mode back to training mode.\n",
    "# y_pred = model(data_train[0][0])\n",
    "# print(y_pred)\n",
    "# print(loss(data_train[0][0],data_train[0][1]))\n",
    "# before_train = loss(y_pred.squeeze(), data_train[0][0].squeeze())\n",
    "# print('Test loss before training' , before_train.item())\n",
    "\n",
    "model.train()\n",
    "\n",
    "loop = tqdm(data_train_loader)\n",
    "for idx, (x, y) in enumerate(loop):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "#     print(y_pred, y)\n",
    "    print('Epoch {}: train loss: {}'.format(idx, loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "absent-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<AbsBackward>) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "n = torch.randint(0,2,(1,), dtype = torch.float, requires_grad = False)\n",
    "print(model(n),n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "choice-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1031, 7.3371])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "def blob_label(y, label, loc): # assign labels\n",
    "    target = numpy.copy(y)\n",
    "    for l in loc:\n",
    "        target[y == l] = label\n",
    "    return target\n",
    "x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
    "x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))\n",
    "\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "complicated-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([0.4343]) tensor([-0.2644]) tensor([-1.])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "n = torch.randint(0,2,(1,))\n",
    "print(n)\n",
    "print(torch.sign(n))\n",
    "n = torch.randn(1)\n",
    "print(m(n),n, torch.sign(n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
